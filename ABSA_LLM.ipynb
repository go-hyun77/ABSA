{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A_m9uWnTdShb5XC3nhaST1LmfR8kSTb8",
      "authorship_tag": "ABX9TyPiR0OT2tdEfMgZeFTSvka2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/go-hyun77/ABSA/blob/f1-scoring/ABSA_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aspect-Based Sentiment Analysis (ABSA) with T5\n",
        "# --------------------------------------------------\n",
        "# This notebook shows how to fine-tune a T5 model for ABSA using HuggingFace.\n",
        "# SemEval2014 dataset (aspect + sentiment annotations).\n",
        "\n",
        "!pip install transformers datasets sentencepiece -q\n",
        "!pip install datasets==3.6.0\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L5d8yE95AOwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d33004-88ad-41bb-8f56-a28570eeaa62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "dataset = load_dataset(\"alexcadillon/SemEval2014Task4\", \"restaurants\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BlHGImlhFNu",
        "outputId": "32f07c43-0f84-4d25-e73c-4303e57f6eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examine dataset\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# print first 10 entries of train split\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}: {train_data[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_IQMpuJsjh",
        "outputId": "65fc4966-69e6-497d-98c6-1627eec4ad81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: {'sentenceId': '3121', 'text': 'But the staff was so horrible to us.', 'aspectTerms': [{'term': 'staff', 'polarity': 'negative', 'from': '8', 'to': '13'}], 'aspectCategories': [{'category': 'service', 'polarity': 'negative'}]}\n",
            "2: {'sentenceId': '2777', 'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '57', 'to': '61'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'anecdotes/miscellaneous', 'polarity': 'negative'}]}\n",
            "3: {'sentenceId': '1634', 'text': \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '4', 'to': '8'}, {'term': 'kitchen', 'polarity': 'positive', 'from': '55', 'to': '62'}, {'term': 'menu', 'polarity': 'neutral', 'from': '141', 'to': '145'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n",
            "4: {'sentenceId': '2534', 'text': 'Where Gabriela personaly greets you and recommends you what to eat.', 'aspectTerms': [], 'aspectCategories': [{'category': 'service', 'polarity': 'positive'}]}\n",
            "5: {'sentenceId': '583', 'text': \"For those that go once and don't enjoy it, all I can say is that they just don't get it.\", 'aspectTerms': [], 'aspectCategories': [{'category': 'anecdotes/miscellaneous', 'polarity': 'positive'}]}\n",
            "6: {'sentenceId': '2846', 'text': \"Not only was the food outstanding, but the little 'perks' were great.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '17', 'to': '21'}, {'term': 'perks', 'polarity': 'positive', 'from': '51', 'to': '56'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'service', 'polarity': 'positive'}]}\n",
            "7: {'sentenceId': '1571', 'text': 'It is very overpriced and not very tasty.', 'aspectTerms': [], 'aspectCategories': [{'category': 'food', 'polarity': 'negative'}, {'category': 'price', 'polarity': 'negative'}]}\n",
            "8: {'sentenceId': '1458', 'text': 'Our agreed favorite is the orrechiete with sausage and chicken (usually the waiters are kind enough to split the dish in half so you get to sample both meats).', 'aspectTerms': [{'term': 'orrechiete with sausage and chicken', 'polarity': 'positive', 'from': '27', 'to': '62'}, {'term': 'waiters', 'polarity': 'positive', 'from': '76', 'to': '83'}, {'term': 'meats', 'polarity': 'neutral', 'from': '152', 'to': '157'}, {'term': 'dish', 'polarity': 'neutral', 'from': '113', 'to': '117'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'service', 'polarity': 'positive'}]}\n",
            "9: {'sentenceId': '3161', 'text': 'The Bagels have an outstanding taste with a terrific texture, both chewy yet not gummy.', 'aspectTerms': [{'term': 'Bagels', 'polarity': 'positive', 'from': '4', 'to': '10'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n",
            "10: {'sentenceId': '2391', 'text': 'Nevertheless the food itself is pretty good.', 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '17', 'to': '21'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten dataset\n",
        "indexes = [train_data[i] for i in range(20)]  # first 20 entries\n",
        "\n",
        "\n",
        "rows = []\n",
        "for i in indexes:\n",
        "    sentence_id = i[\"sentenceId\"]\n",
        "    text = i[\"text\"]\n",
        "\n",
        "    # If aspect terms exist, iterate through them\n",
        "    if i[\"aspectTerms\"]:\n",
        "        for asp in i[\"aspectTerms\"]:\n",
        "            rows.append({\n",
        "                \"sentenceId\": sentence_id,\n",
        "                \"text\": text,\n",
        "                \"aspect_term\": asp[\"term\"],\n",
        "                \"term_polarity\": asp[\"polarity\"],\n",
        "                \"category\": None,  # Add these to maintain consistent columns\n",
        "                \"category_polarity\": None # Add these to maintain consistent columns\n",
        "            })\n",
        "    # If no explicit aspect terms, still record categories\n",
        "    if i[\"aspectCategories\"]:\n",
        "        for cat in i[\"aspectCategories\"]:\n",
        "            rows.append({\n",
        "                \"sentenceId\": sentence_id,\n",
        "                \"text\": text,\n",
        "                \"aspect_term\": None, # Add these to maintain consistent columns\n",
        "                \"term_polarity\": None, # Add these to maintain consistent columns\n",
        "                \"category\": cat[\"category\"],\n",
        "                \"category_polarity\": cat[\"polarity\"]\n",
        "            })\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a3uLdYKqBcy",
        "outputId": "a4b308fc-cbcc-4d4a-a372-79c83eb55dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sentenceId                                               text aspect_term  \\\n",
            "0       3121               But the staff was so horrible to us.       staff   \n",
            "1       3121               But the staff was so horrible to us.        None   \n",
            "2       2777  To be completely fair, the only redeeming fact...        food   \n",
            "3       2777  To be completely fair, the only redeeming fact...        None   \n",
            "4       2777  To be completely fair, the only redeeming fact...        None   \n",
            "5       1634  The food is uniformly exceptional, with a very...        food   \n",
            "6       1634  The food is uniformly exceptional, with a very...     kitchen   \n",
            "7       1634  The food is uniformly exceptional, with a very...        menu   \n",
            "8       1634  The food is uniformly exceptional, with a very...        None   \n",
            "9       2534  Where Gabriela personaly greets you and recomm...        None   \n",
            "\n",
            "  term_polarity                 category category_polarity  \n",
            "0      negative                     None              None  \n",
            "1          None                  service          negative  \n",
            "2      positive                     None              None  \n",
            "3          None                     food          positive  \n",
            "4          None  anecdotes/miscellaneous          negative  \n",
            "5      positive                     None              None  \n",
            "6      positive                     None              None  \n",
            "7       neutral                     None              None  \n",
            "8          None                     food          positive  \n",
            "9          None                  service          positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "\n",
        "model_name = \"t5-small\" #try \"google/flan-t5-base\" for better results\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChew2YNg34W",
        "outputId": "3fceaabf-fb66-43fe-baf4-89c993fc692b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create aspect-sentiment pairs from dataset\n",
        "\n",
        "def format_target(ex):\n",
        "    pairs = []\n",
        "    for asp in ex.get(\"aspectTerms\", []):\n",
        "        term = asp[\"term\"]\n",
        "        pol = asp[\"polarity\"]\n",
        "        pairs.append(f\"{term}: {pol}\")\n",
        "    return \"; \".join(pairs) if pairs else \"no aspects\"\n"
      ],
      "metadata": {
        "id": "lodARW4LCsfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to tokenize inputs (as in the plain sentences + aspect terms/values) for model to train on\n",
        "\n",
        "def preprocess(ex):\n",
        "    input_text = f\"ABSA: {ex['text']}\"\n",
        "    target_text = format_target(ex)\n",
        "    return tokenizer(\n",
        "        input_text,\n",
        "        text_target=target_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )"
      ],
      "metadata": {
        "id": "p8b5teV4753k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply preprocess function to each entry in training and validation test splits\n",
        "train_dataset = dataset[\"train\"].map(preprocess)\n",
        "valid_dataset = dataset[\"test\"].map(preprocess)\n"
      ],
      "metadata": {
        "id": "RMj1wkxv8GAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM2FeoYK9eIX",
        "outputId": "5818dd52-713c-4f2a-d20b-1cc544871dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentenceId': '3121', 'text': 'But the staff was so horrible to us.', 'aspectTerms': [{'term': 'staff', 'polarity': 'negative', 'from': '8', 'to': '13'}], 'aspectCategories': [{'category': 'service', 'polarity': 'negative'}], 'input_ids': [20798, 188, 10, 299, 8, 871, 47, 78, 17425, 12, 178, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [871, 10, 2841, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "X-DNhMzq8lpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training setup and parameters\n",
        "\n",
        "args = TrainingArguments(\n",
        "  output_dir=\"./absa_t5\",\n",
        "  eval_strategy=\"epoch\", # Corrected parameter name\n",
        "  learning_rate=5e-5,\n",
        "  per_device_train_batch_size=8,\n",
        "  num_train_epochs=5,\n",
        "  weight_decay=0.01,\n",
        "  save_total_limit=2,\n",
        "  logging_steps=50,\n",
        "  push_to_hub=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=valid_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "IOKfZll08uRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model, no need to execute this block if loading saved model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "2tQVDzKFFTIV",
        "outputId": "dd11db6b-1316-4cb1-c212-667ecf55d75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgohyun\u001b[0m (\u001b[33mgohyun-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251109_055335-xgmk0sr3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gohyun-california-state-university-fullerton/huggingface/runs/xgmk0sr3' target=\"_blank\">stoic-firefly-7</a></strong> to <a href='https://wandb.ai/gohyun-california-state-university-fullerton/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gohyun-california-state-university-fullerton/huggingface' target=\"_blank\">https://wandb.ai/gohyun-california-state-university-fullerton/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gohyun-california-state-university-fullerton/huggingface/runs/xgmk0sr3' target=\"_blank\">https://wandb.ai/gohyun-california-state-university-fullerton/huggingface/runs/xgmk0sr3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1905' max='1905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1905/1905 6:43:10, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.041716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.039300</td>\n",
              "      <td>0.028546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.025298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.034200</td>\n",
              "      <td>0.024151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>0.023684</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1905, training_loss=0.18660602854305677, metrics={'train_runtime': 24234.8924, 'train_samples_per_second': 0.627, 'train_steps_per_second': 0.079, 'total_flos': 514468022845440.0, 'train_loss': 0.18660602854305677, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive folder for saving trained model\n",
        "#!fusermount -u /content/drive\n",
        "#!rm -rf /content/drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/ABSA_T5_Model\"\n",
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "F4NfMCNTDYuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c574ba27-6916-4e0c-f674-e200f754dcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'3rd Iteration Document'  'CPSC 301'\t  'CPSC 439'  'CPSC 566'\n",
            " ABSA_T5_Model\t\t  'CPSC 311'\t  'CPSC 440'  'CPSC 585'\n",
            "'AP GOV'\t\t  'CPSC 315'\t  'CPSC 452'  'CPSC 589'\n",
            " BIO101\t\t\t  'CPSC 323'\t  'CPSC 471'  'EGCP 401'\n",
            " Books\t\t\t  'CPSC 332'\t  'CPSC 481'  'EVO Food Places.xlsx'\n",
            "'Colab Notebooks'\t  'CPSC 335'\t  'CPSC 485'   MATH338\n",
            "'CPSC 121'\t\t  'CPSC 351'\t  'CPSC 531'   Misc.\n",
            "'CPSC 223J'\t\t  'CPSC 353 458'  'CPSC 544'  'Oct Genesis.png'\n",
            "'CPSC 240'\t\t  'CPSC 362'\t  'CPSC 548'  'PSC Biotech'\n",
            "'CPSC 254'\t\t  'CPSC 375'\t  'CPSC 552'  'Test Folder'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)\n"
      ],
      "metadata": {
        "id": "k__5iOxntBSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa2e693-54ea-47ed-c07c-60643ab07189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ABSA_T5_Model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/spiece.model',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model from your Drive\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir, local_files_only=True)\n",
        "\n",
        "\n",
        "print(\"Model path:\", model.config._name_or_path)\n",
        "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()) // 1e6, \"M\")"
      ],
      "metadata": {
        "id": "I-aYS7FLAcub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63db353a-a097-4b05-84b0-2d3332327395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/drive/MyDrive/ABSA_T5_Model\n",
            "Number of parameters: 60.0 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test model with text input\n",
        "\n",
        "# 1️⃣ Confirm model path\n",
        "print(\"Model path:\", model.config._name_or_path)\n",
        "\n",
        "# 2️⃣ Confirm the prefix was used during training\n",
        "print(\"Example training input:\", dataset[\"train\"][0][\"text\"])\n",
        "\n",
        "# 3️⃣ Try inference without prefix (if you didn't train with one)\n",
        "def absa_predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#pass sample text input to test absa\n",
        "print(absa_predict(\"The food was amazing but the service was terrible.\"))\n"
      ],
      "metadata": {
        "id": "vbnjIu1xcAH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef541b4b-2acc-4d22-a873-8cd22abfebd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/drive/MyDrive/ABSA_T5_Model\n",
            "Example training input: But the staff was so horrible to us.\n",
            "food was positive; service was negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#parse absa text outputs into structured data\n",
        "\n",
        "def parse_output(output):\n",
        "    pairs = []\n",
        "    segments = output.split(\";\")\n",
        "    for seg in segments:\n",
        "        seg = seg.strip()\n",
        "\n",
        "        # Try to match \"aspect: sentiment\" format\n",
        "        match_colon = re.match(r\"(.+?):\\s*(positive|negative|neutral)\", seg)\n",
        "        if match_colon:\n",
        "            aspect = match_colon.group(1).strip()\n",
        "            sentiment = match_colon.group(2).strip()\n",
        "            pairs.append({\"aspect\": aspect, \"sentiment\": sentiment})\n",
        "        # Try to match \"aspect was sentiment\" format\n",
        "        elif \" was \" in seg:\n",
        "            parts = seg.split(\" was \")\n",
        "            if len(parts) == 2: # Ensure it splits into exactly two parts\n",
        "                aspect = parts[0].strip()\n",
        "                sentiment = parts[1].strip()\n",
        "                pairs.append({\"aspect\": aspect, \"sentiment\": sentiment})\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "duyKF5kwZ_eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1 score compute function\n",
        "\n",
        "def compute_f1(true_pairs, pred_pairs):\n",
        "    # Convert lists of dicts → sets of tuples\n",
        "    true_set = set((p[\"aspect\"], p[\"sentiment\"]) for p in true_pairs)\n",
        "    pred_set = set((p[\"aspect\"], p[\"sentiment\"]) for p in pred_pairs)\n",
        "\n",
        "    TP = len(true_set & pred_set)\n",
        "    FP = len(pred_set - true_set)\n",
        "    FN = len(true_set - pred_set)\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-8)\n",
        "    recall = TP / (TP + FN + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "metadata": {
        "id": "REUF3q60aNCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    total_p, total_r, total_f1 = 0, 0, 0\n",
        "    count = 0\n",
        "\n",
        "    for ex in tqdm(dataset):\n",
        "        # Use format_target to get the true string labels from the original aspectTerms\n",
        "        true_pairs_str = format_target(ex)\n",
        "        true_pairs = parse_output(true_pairs_str)\n",
        "\n",
        "        pred = absa_predict(ex[\"text\"])\n",
        "        pred_pairs = parse_output(pred)\n",
        "\n",
        "        p, r, f1 = compute_f1(true_pairs, pred_pairs)\n",
        "        total_p += p\n",
        "        total_r += r\n",
        "        total_f1 += f1\n",
        "        count += 1\n",
        "\n",
        "    return {\n",
        "        \"precision\": total_p / count,\n",
        "        \"recall\": total_r / count,\n",
        "        \"f1\": total_f1 / count\n",
        "    }"
      ],
      "metadata": {
        "id": "Va8t2dNRaSCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = evaluate_model(model, tokenizer, valid_dataset)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDfMMqSacPS",
        "outputId": "53536a66-febd-4b2b-820d-f85207e8d97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [10:28<00:00,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.13749999896416676, 'recall': 0.13395833231982654, 'f1': 0.13286011731101147}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}