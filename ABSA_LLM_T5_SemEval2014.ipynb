{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1A_m9uWnTdShb5XC3nhaST1LmfR8kSTb8",
      "authorship_tag": "ABX9TyNZwBJ4/5SzXqSSrgOH3R6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/go-hyun77/ABSA/blob/documentation-and-refactor/ABSA_LLM_T5_SemEval2014.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Aspect-Based Sentiment Analysis (ABSA) with T5</u>**\n",
        "This notebook implements a **T5 (Text-to-Text Transfer Transformer)** LLM capable of performing aspect-based sentiment analysis on the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4). Instructions/explanations for testing/reviewing each code block will be outlined for posterity purposes. This model aims to perform aspect-based sentiment analysis by way of aspect extraction and sentiment classification from parsed text.\n",
        "\n",
        "> Aspect-based Sentiment Analysis (ABSA) is more nuanced form of sentiment analysis where specific aspects (features or topics) within a text are identified and mapped to the sentiment expressed towards said aspect.\n",
        "\n",
        "The expected input/outputs of this model will be as follows in the given example:\n",
        "```\n",
        "INPUT: BEST spicy tuna roll, great asian salad.\n",
        "```\n",
        "```\n",
        "OUTPUT: aspect=tuna roll, sentiment=positive; aspect=asian salad, sentiment=positive\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bOoFUSJbIR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies and import libraries\n",
        "!pip install transformers datasets sentencepiece -q\n",
        "!pip install datasets==3.6.0\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive') #mount drive for saving/loading model\n",
        "model_dir = \"/content/drive/MyDrive/ABSA_T5_Model\" #define model directory in google drive, you may need to modify this link to point to the appropriate directory"
      ],
      "metadata": {
        "id": "L5d8yE95AOwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d816d5-bf8b-4044-8af4-ba04fdcdbe37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==3.6.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-3.6.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Defining the Model (First Run)</u>**\n",
        "The **T5 (Text-to-Text Transfer Transformer)** LLM is chosen as the baseline model in which we evaluate its capabilities to perform ABSA following subsequent training on the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4). The T5 LLM is well suited to the task of ABSA as its defining idea is that everything is treated as a text-to-text problem, meaning that all inputs are text, and all outputs are text.\n",
        ">If you would like to train the model from scratch to perform ABSA, execute the following block to select the T5-small model as a baseline to begin training on.\n"
      ],
      "metadata": {
        "id": "bYYALqo9SJAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define t5 base model\n",
        "\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "RChew2YNg34W"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Defining the Model (Loading Saved Model)</u>**\n",
        "If you are resuming from a previous session in which you have successfully trained a model on the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4), execute this block to load the model configs to continue. Otherwise, make sure to load the base model in the previous block to continue."
      ],
      "metadata": {
        "id": "g0nqj87mCSY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer and model from Drive\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir, local_files_only=True)\n",
        "\n",
        "\n",
        "print(\"Model path:\", model.config._name_or_path)  #sanity check, print model path\n",
        "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()) // 1e6, \"M\")  #sanity check, check for model params to verify successful load"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfaab3c-86ef-4e81-fe45-d1efbc46ced9",
        "id": "pKPcqtwVCFm4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/drive/MyDrive/ABSA_T5_Model\n",
            "Number of parameters: 60.0 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Loading and Examining the Dataset</u>**\n",
        "Below we load and examine the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4), minimal preprocessing is done to enhance the readability of the outputted columns and maintain consistency when no values are present for a specific column (for example when a polarity is not present due to lack of either positive or negative connotation). <br>\n",
        "\n",
        "In this project, we will load and work with the restaurant review specific portion of the dataset with the following line:\n",
        "```\n",
        "dataset = load_dataset(\"alexcadillon/SemEval2014Task4\", \"restaurants\")\n",
        "```\n",
        "The [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4)'s data columns contain the following attributes outlined in the below table:\n",
        "\n",
        "| Field Name | Data Type | Description |\n",
        "| :------- | :------: | -------: |\n",
        "| sentenceId  | string  | Unique ID of the sentence mainly used for identification purposes.  |\n",
        "| text  | string | The actual raw text content of the sentence.  |\n",
        "| aspectTerms | list of dicts | Attributes for a given aspect: term, polarity, from (offset start), to (offset end).  |\n",
        "| aspectCategories  | list of dicts | Categorical annotations for a given aspect and its respective polarity. |\n",
        "| domain  | class label | Identifier for which domain this entry belongs to (restaurants in this case). |\n"
      ],
      "metadata": {
        "id": "K7eGQ4xFpUJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "\n",
        "dataset = load_dataset(\"alexcadillon/SemEval2014Task4\", \"restaurants\") #restaurant reviews dataset config"
      ],
      "metadata": {
        "id": "8BlHGImlhFNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examine dataset\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# print first 10 entries of train split\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}: {train_data[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_IQMpuJsjh",
        "outputId": "f89e8eb0-2af9-499d-daed-b520c4ddb184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: {'sentenceId': '3121', 'text': 'But the staff was so horrible to us.', 'aspectTerms': [{'term': 'staff', 'polarity': 'negative', 'from': '8', 'to': '13'}], 'aspectCategories': [{'category': 'service', 'polarity': 'negative'}]}\n",
            "2: {'sentenceId': '2777', 'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '57', 'to': '61'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'anecdotes/miscellaneous', 'polarity': 'negative'}]}\n",
            "3: {'sentenceId': '1634', 'text': \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '4', 'to': '8'}, {'term': 'kitchen', 'polarity': 'positive', 'from': '55', 'to': '62'}, {'term': 'menu', 'polarity': 'neutral', 'from': '141', 'to': '145'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n",
            "4: {'sentenceId': '2534', 'text': 'Where Gabriela personaly greets you and recommends you what to eat.', 'aspectTerms': [], 'aspectCategories': [{'category': 'service', 'polarity': 'positive'}]}\n",
            "5: {'sentenceId': '583', 'text': \"For those that go once and don't enjoy it, all I can say is that they just don't get it.\", 'aspectTerms': [], 'aspectCategories': [{'category': 'anecdotes/miscellaneous', 'polarity': 'positive'}]}\n",
            "6: {'sentenceId': '2846', 'text': \"Not only was the food outstanding, but the little 'perks' were great.\", 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '17', 'to': '21'}, {'term': 'perks', 'polarity': 'positive', 'from': '51', 'to': '56'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'service', 'polarity': 'positive'}]}\n",
            "7: {'sentenceId': '1571', 'text': 'It is very overpriced and not very tasty.', 'aspectTerms': [], 'aspectCategories': [{'category': 'food', 'polarity': 'negative'}, {'category': 'price', 'polarity': 'negative'}]}\n",
            "8: {'sentenceId': '1458', 'text': 'Our agreed favorite is the orrechiete with sausage and chicken (usually the waiters are kind enough to split the dish in half so you get to sample both meats).', 'aspectTerms': [{'term': 'orrechiete with sausage and chicken', 'polarity': 'positive', 'from': '27', 'to': '62'}, {'term': 'waiters', 'polarity': 'positive', 'from': '76', 'to': '83'}, {'term': 'meats', 'polarity': 'neutral', 'from': '152', 'to': '157'}, {'term': 'dish', 'polarity': 'neutral', 'from': '113', 'to': '117'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}, {'category': 'service', 'polarity': 'positive'}]}\n",
            "9: {'sentenceId': '3161', 'text': 'The Bagels have an outstanding taste with a terrific texture, both chewy yet not gummy.', 'aspectTerms': [{'term': 'Bagels', 'polarity': 'positive', 'from': '4', 'to': '10'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n",
            "10: {'sentenceId': '2391', 'text': 'Nevertheless the food itself is pretty good.', 'aspectTerms': [{'term': 'food', 'polarity': 'positive', 'from': '17', 'to': '21'}], 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten dataset for readability\n",
        "indexes = [train_data[i] for i in range(20)]  # first 20 entries\n",
        "\n",
        "rows = []\n",
        "for i in indexes:\n",
        "    sentence_id = i[\"sentenceId\"]\n",
        "    text = i[\"text\"]\n",
        "\n",
        "    # If aspect terms exist, iterate through them\n",
        "    if i[\"aspectTerms\"]:\n",
        "        for asp in i[\"aspectTerms\"]:\n",
        "            rows.append({\n",
        "                \"sentenceId\": sentence_id,\n",
        "                \"text\": text,\n",
        "                \"aspect_term\": asp[\"term\"],\n",
        "                \"term_polarity\": asp[\"polarity\"],\n",
        "                \"category\": None,  # Add these to maintain consistent columns\n",
        "                \"category_polarity\": None # Add these to maintain consistent columns\n",
        "            })\n",
        "    # If no explicit aspect terms, still record categories\n",
        "    if i[\"aspectCategories\"]:\n",
        "        for cat in i[\"aspectCategories\"]:\n",
        "            rows.append({\n",
        "                \"sentenceId\": sentence_id,\n",
        "                \"text\": text,\n",
        "                \"aspect_term\": None, # Add these to maintain consistent columns\n",
        "                \"term_polarity\": None, # Add these to maintain consistent columns\n",
        "                \"category\": cat[\"category\"],\n",
        "                \"category_polarity\": cat[\"polarity\"]\n",
        "            })\n",
        "\n",
        "\n",
        "#convert to DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a3uLdYKqBcy",
        "outputId": "ce2172e9-c367-4314-e6d3-b1b14afb3a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sentenceId                                               text aspect_term  \\\n",
            "0       3121               But the staff was so horrible to us.       staff   \n",
            "1       3121               But the staff was so horrible to us.        None   \n",
            "2       2777  To be completely fair, the only redeeming fact...        food   \n",
            "3       2777  To be completely fair, the only redeeming fact...        None   \n",
            "4       2777  To be completely fair, the only redeeming fact...        None   \n",
            "5       1634  The food is uniformly exceptional, with a very...        food   \n",
            "6       1634  The food is uniformly exceptional, with a very...     kitchen   \n",
            "7       1634  The food is uniformly exceptional, with a very...        menu   \n",
            "8       1634  The food is uniformly exceptional, with a very...        None   \n",
            "9       2534  Where Gabriela personaly greets you and recomm...        None   \n",
            "\n",
            "  term_polarity                 category category_polarity  \n",
            "0      negative                     None              None  \n",
            "1          None                  service          negative  \n",
            "2      positive                     None              None  \n",
            "3          None                     food          positive  \n",
            "4          None  anecdotes/miscellaneous          negative  \n",
            "5      positive                     None              None  \n",
            "6      positive                     None              None  \n",
            "7       neutral                     None              None  \n",
            "8          None                     food          positive  \n",
            "9          None                  service          positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Preparing the Dataset for Training</u>**\n",
        "As we previously saw in the above block, the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4) consists of structured labels, columns, and values.\n",
        "```\n",
        " sentenceId                                               text aspect_term  \\\n",
        "0       3121               But the staff was so horrible to us.       staff   \n",
        "1       3121               But the staff was so horrible to us.        None   \n",
        "2       2777  To be completely fair, the only redeeming fact...        food   \n",
        "3       2777  To be completely fair, the only redeeming fact...        None   \n",
        "\n",
        "  term_polarity                 category category_polarity  \n",
        "0      negative                     None              None  \n",
        "1          None                  service          negative  \n",
        "2      positive                     None              None  \n",
        "3          None                     food          positive  \n",
        "```\n",
        "On its own, it is not in a format that T5 can **accept** and **produce**. Thus, the input to T5 will be formatted into a raw text aspect-sentiment \"pair\" within the structure seen as follows from:\n",
        "```\n",
        "\"aspectTerms\": [\n",
        "    {\"term\": \"staff\", \"polarity\": \"negative\"}\n",
        "  ]\n",
        "```\n",
        "to the following:\n",
        "\n",
        "```\n",
        "aspect=staff, sentiment=negative\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nNd7Umc7vewD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create aspect-sentiment pairs from dataset such as \"aspect=food, sentiment=positive; aspect=service, sentiment=negative\" or \"none\" if no aspects\n",
        "import json\n",
        "\n",
        "def format_target(ex):\n",
        "\n",
        "    pairs = []\n",
        "    for asp in ex.get(\"aspectTerms\", []): #for all aspects from dataset column aspectTerms, pull term label and sentiment polarity\n",
        "        term = asp.get(\"term\")\n",
        "        pol = asp.get(\"polarity\")\n",
        "        if term is None or pol is None: #if none\n",
        "            continue\n",
        "        #normalize to lowercase and remove whitespace\n",
        "        pairs.append(f\"aspect={term.strip()}, sentiment={pol.strip().lower()}\")\n",
        "    return \"; \".join(pairs) if pairs else \"none\"\n",
        "\n"
      ],
      "metadata": {
        "id": "lodARW4LCsfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to tokenize inputs (as in the plain sentences + aspect terms/values) for model to train on\n",
        "\n",
        "def preprocess(ex):\n",
        "\n",
        "    #explicit task instruction to prevent input echoing and training collapse\n",
        "    instruction = (\n",
        "        \"Extract aspect-based sentiment. \"\n",
        "        \"Return outputs in the exact format: \"\n",
        "        \"'aspect=<term>, sentiment=<positive|negative|neutral>' \"\n",
        "        \"separated by '; ' for multiple aspects. If no aspects, return 'none'.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    #include instruction in the input\n",
        "    input_text = instruction + \"ABSA: \" + ex[\"text\"]\n",
        "\n",
        "    target_text = format_target(ex)\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        input_text,\n",
        "        text_target=target_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Add raw text and aspectTerms to the processed example for evaluation\n",
        "    model_inputs[\"raw_text\"] = ex[\"text\"]\n",
        "    model_inputs[\"raw_aspects\"] = ex.get(\"aspectTerms\", [])\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "p8b5teV4753k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply preprocess function to each entry in training and validation test splits\n",
        "train_dataset = dataset[\"train\"].map(preprocess, remove_columns=[])\n",
        "valid_dataset = dataset[\"test\"].map(preprocess, remove_columns=[])"
      ],
      "metadata": {
        "id": "RMj1wkxv8GAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PyTorch format (so Trainer can use them directly)\n",
        "train_dataset.set_format(type=\"torch\")\n",
        "valid_dataset.set_format(type=\"torch\")"
      ],
      "metadata": {
        "id": "-di-PG1c3sNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Pre-Training Validation Checks</u>**\n",
        "Below are validation and sanity checks to ensure everything is prepared for and to avoid unnecessary training (as the total training time spent was approximately 5-6 hours for 5 epochs)."
      ],
      "metadata": {
        "id": "CeuDkxzSH1GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#quick verify format_target on one raw example to see whether aspect-sentiment pair was generated properly from the above format_target()\n",
        "\n",
        "#expected output: aspect=staff, sentiment=negative\n",
        "print(\"RAW example:\", dataset[\"train\"][0])\n",
        "print(\"FORMATTED target:\", format_target(dataset[\"train\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqGNIucy3ReA",
        "outputId": "97d264be-c0b3-4c79-8838-2d8d79453f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW example: {'sentenceId': '3121', 'text': 'But the staff was so horrible to us.', 'aspectTerms': [{'term': 'staff', 'polarity': 'negative', 'from': '8', 'to': '13'}], 'aspectCategories': [{'category': 'service', 'polarity': 'negative'}]}\n",
            "FORMATTED target: aspect=staff, sentiment=negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#quick decode check (ensure tokenizer didn't strip or change the target format)\n",
        "\n",
        "#expected output: aspect=staff, sentiment=negative\n",
        "print(\"Decoded input (train[0]):\")\n",
        "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))\n",
        "print(\"Decoded target (train[0]):\")\n",
        "print(tokenizer.decode(train_dataset[0][\"labels\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwE9q7r03xWi",
        "outputId": "f7bb03bf-6c3c-4d2e-9254-cada0bb3ed61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded input (train[0]):\n",
            "Extract aspect-based sentiment. Return outputs in the exact format: 'aspect=term>, sentiment=positive|negative|neutral>' separated by ';'for multiple aspects. If no aspects, return 'none'. ABSA: But the staff was so horrible to us.\n",
            "Decoded target (train[0]):\n",
            "aspect=staff, sentiment=negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, labels of first index of training data set\n",
        "print(tokenizer.decode(train_dataset[0][\"labels\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM2FeoYK9eIX",
        "outputId": "6a957386-02b9-41b2-c237-324f29c53265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aspect=staff, sentiment=negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, input of training data post-preprocessing, must include instruction and \"ABSA:\" prefix\n",
        "\n",
        "#expected output: <instruction> ABSA: <input>\n",
        "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJuiWrOB0vjq",
        "outputId": "f85656a5-e888-4ff9-88eb-a55f11c2245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract aspect-based sentiment. Return outputs in the exact format: 'aspect=term>, sentiment=positive|negative|neutral>' separated by ';'for multiple aspects. If no aspects, return 'none'. ABSA: But the staff was so horrible to us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, load t5 base model to confirm base model is loaded for training\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "X-DNhMzq8lpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, verify model directory for saving model configs\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/ABSA_T5_Model\"\n",
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "F4NfMCNTDYuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d389b0-1827-4d1b-e13c-717e5666f9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'3rd Iteration Document'    'CPSC 301'\t    'CPSC 440'\t'CPSC 589'\n",
            " ABSA_T5_Model\t\t    'CPSC 311'\t    'CPSC 452'\t'EGCP 401'\n",
            " ABSA_T5_Model_New_Dataset  'CPSC 315'\t    'CPSC 471'\t'EVO Food Places.xlsx'\n",
            "'AP GOV'\t\t    'CPSC 323'\t    'CPSC 481'\t MATH338\n",
            " BIO101\t\t\t    'CPSC 332'\t    'CPSC 485'\t Misc.\n",
            " Books\t\t\t    'CPSC 335'\t    'CPSC 531'\t'Oct Genesis.png'\n",
            "'Colab Notebooks'\t    'CPSC 351'\t    'CPSC 544'\t'PSC Biotech'\n",
            "'CPSC 121'\t\t    'CPSC 353 458'  'CPSC 548'\t'Test Folder'\n",
            "'CPSC 223J'\t\t    'CPSC 362'\t    'CPSC 552'\n",
            "'CPSC 240'\t\t    'CPSC 375'\t    'CPSC 566'\n",
            "'CPSC 254'\t\t    'CPSC 439'\t    'CPSC 585'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Model Training</u>**\n",
        "This block contains code to initialize training parameters, execute training, and output training results. You may skip this block if you are loading a previously trained model from Drive.\n",
        ">**NOTE:** There is a rather long training time required overall due to the size of the [SemEval2014 dataset](https://huggingface.co/datasets/alexcadillon/SemEval2014Task4)'s restaurant review split. While model accuracy has been observed to improve with a larger number of epochs in prior testing, in the interest of time the number of epochs has been limited to 5. You may increase this number to observe/test validation accuracy changes at your discretion."
      ],
      "metadata": {
        "id": "QJXg59tbK6pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training setup and parameters\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    #directory where checkpoints, logs, and metadata are saved\n",
        "    output_dir=\"./absa_t5\",\n",
        "\n",
        "    #when evaluation occurs, in this case once in beginning of each epoch\n",
        "    eval_strategy=\"epoch\",\n",
        "\n",
        "    #how often checkpoints are saved, in this case once in beginning of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    #learning rate, recommended for t5-small is between 1e-4 → 3e-5 for AdamW\n",
        "    learning_rate=3e-5,\n",
        "\n",
        "    #increasing batch size per gpu for training/evaluation stabilizes gradients, reduces training time, but increases GPU memory requirements\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "\n",
        "    #num of passes through the dataset\n",
        "    num_train_epochs=6,\n",
        "\n",
        "    #regularization to reduce overfitting\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    #number of checkpoints saved (last two epochs)\n",
        "    save_total_limit=2,\n",
        "\n",
        "    #how often training logs are printed (every 50 steps)\n",
        "    logging_steps=50,\n",
        "\n",
        "    #gradient computation, set to false as unsupported by GPU\n",
        "    fp16=False,    # set True if your GPU supports it\n",
        "\n",
        "    #toggle auto push to huggingface\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "#constructor\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=valid_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "IOKfZll08uRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model, no need to execute this block if loading saved model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "2tQVDzKFFTIV",
        "outputId": "1b0bd0b7-5902-437b-a081-2f7327cf59d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2286' max='2286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2286/2286 6:56:58, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.163200</td>\n",
              "      <td>0.086505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.076500</td>\n",
              "      <td>0.058833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.069300</td>\n",
              "      <td>0.051112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.071300</td>\n",
              "      <td>0.048222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.046494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.056700</td>\n",
              "      <td>0.045654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2286, training_loss=0.3016708865044728, metrics={'train_runtime': 25072.7909, 'train_samples_per_second': 0.728, 'train_steps_per_second': 0.091, 'total_flos': 617361627414528.0, 'train_loss': 0.3016708865044728, 'epoch': 6.0})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)\n"
      ],
      "metadata": {
        "id": "k__5iOxntBSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07278b44-a2e2-4638-e0a3-012679edbcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ABSA_T5_Model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/spiece.model',\n",
              " '/content/drive/MyDrive/ABSA_T5_Model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Post-Training Validation Checks</u>**\n",
        "This block contains code to validate training inputs, and decoded training."
      ],
      "metadata": {
        "id": "P0bn0T6CSWyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, confirm correct model input post pre-processing\n",
        "print(\"Decoded training input:\", tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW4QLLyIHCbG",
        "outputId": "74511fc4-28b6-4882-8dd3-905a2a05fc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded training input: Extract aspect-based sentiment. Return outputs in the exact format: 'aspect=term>, sentiment=positive|negative|neutral>' separated by ';'for multiple aspects. If no aspects, return 'none'. ABSA: But the staff was so horrible to us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, confirm correct label post pre-processing\n",
        "print(\"Decoded training label:\", tokenizer.decode(train_dataset[0][\"labels\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs_Se2sWGy6n",
        "outputId": "d8d473d7-6de6-418c-bd19-047e375c33a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded training label: aspect=staff, sentiment=negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, confirm model path\n",
        "print(\"Model path:\", model.config._name_or_path)\n",
        "\n",
        "#sanity check, confirm the ABSA: prefix was used during training\n",
        "print(\"Example training input:\", dataset[\"train\"][0][\"text\"])\n",
        "\n",
        "#try inference without prefix (if you didn't train with one)\n",
        "def absa_predict(text):\n",
        "    instruction = (\n",
        "        \"Extract aspect-based sentiment. \"\n",
        "        \"Return outputs in the exact format: \"\n",
        "        \"'aspect=<term>, sentiment=<positive|negative|neutral>' \"\n",
        "        \"separated by '; ' for multiple aspects. If no aspects, return 'none'.\\n\\n\"\n",
        "    )\n",
        "    full_input = instruction + \"ABSA: \" + text\n",
        "\n",
        "    inputs = tokenizer(full_input, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "vbnjIu1xcAH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ca23d2-9c1d-4c0f-d853-2155ba9a1186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/drive/MyDrive/ABSA_T5_Model\n",
            "Example training input: But the staff was so horrible to us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>ABSA Input Tests</u>**\n",
        "This block contains the function to test the trained model's functional accuracy by manually inputting text to gauge the correctness of its evaluation. Modify the text parameter in the below function and execute the block to have the model identify and output detected aspect-sentiment pairs.\n",
        "```\n",
        "print(absa_predict(\"your-sentence-here\"))\n",
        "```\n"
      ],
      "metadata": {
        "id": "b8awXwkhV5mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test model with text input (you may notice something wrong with this example)\n",
        "print(absa_predict(\"The food was great, but parking was incredibly difficult.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0NMGvYQTpPk",
        "outputId": "cd10f535-d2f6-4be6-8dbe-5648007b08c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aspect=food, sentiment=positive; aspect=parking, sentiment=positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#output first 20 absa results produced by the model on the dataset\n",
        "for i in range(20):\n",
        "    t = dataset[\"test\"][i][\"text\"]\n",
        "    print(\"INPUT:\", t)\n",
        "    print(\"MODEL OUTPUT:\", absa_predict(t))\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr7X85ntwK_J",
        "outputId": "6d43429b-baef-4a97-a035-5ea6c98dfffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT: The bread is top notch as well.\n",
            "MODEL OUTPUT: aspect=bread, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: I have to say they have one of the fastest delivery times in the city.\n",
            "MODEL OUTPUT: aspect=delivery times, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Food is always fresh and hot- ready to eat!\n",
            "MODEL OUTPUT: aspect=Food, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Did I mention that the coffee is OUTSTANDING?\n",
            "MODEL OUTPUT: aspect=caffee, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Certainly not the best sushi in New York, however, it is always fresh, and the place is very clean, sterile.\n",
            "MODEL OUTPUT: aspect=sushi, sentiment=positive; aspect=place, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: I trust the people at Go Sushi, it never disappoints.\n",
            "MODEL OUTPUT: none\n",
            "--------------------------------------------------\n",
            "INPUT: Straight-forward, no surprises, very decent Japanese food.\n",
            "MODEL OUTPUT: aspect=Japan food, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: BEST spicy tuna roll, great asian salad.\n",
            "MODEL OUTPUT: aspect=tuna roll, sentiment=positive; aspect=asian salad, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Try the rose roll (not on menu).\n",
            "MODEL OUTPUT: aspect=rose roll, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: I love the drinks, esp lychee martini, and the food is also VERY good.\n",
            "MODEL OUTPUT: aspect=drinks, sentiment=positive; aspect=food, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: In fact, this was not a Nicoise salad and was barely eatable.\n",
            "MODEL OUTPUT: aspect= Nicoise salad, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: While there's a decent menu, it shouldn't take ten minutes to get your drinks and 45 for a dessert pizza.\n",
            "MODEL OUTPUT: aspect=menu, sentiment=positive; aspect=drinks, sentiment=positive; aspect=delict pizza, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Once we sailed, the top-notch food and live entertainment sold us on a unforgettable evening.\n",
            "MODEL OUTPUT: aspect=food, sentiment=positive; aspect=live entertainment, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Our waiter was horrible; so rude and disinterested.\n",
            "MODEL OUTPUT: aspect=waiter, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: The sangria's - watered down.\n",
            "MODEL OUTPUT: aspect=singrias, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: menu - uneventful, small.\n",
            "MODEL OUTPUT: aspect=menu, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Anytime and everytime I find myself in the neighborhood I will go to Sushi Rose for fresh sushi and great portions all at a reasonable price.\n",
            "MODEL OUTPUT: aspect=sushi, sentiment=positive; aspect=portions, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: Great food but the service was dreadful!\n",
            "MODEL OUTPUT: aspect=food, sentiment=positive; aspect=service, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: The portions of the food that came out were mediocre.\n",
            "MODEL OUTPUT: aspect=portions of food, sentiment=positive\n",
            "--------------------------------------------------\n",
            "INPUT: the two waitress's looked like they had been sucking on lemons.\n",
            "MODEL OUTPUT: aspect=waitress, sentiment=positive\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>F1 Score Evaluation</u>**\n",
        "Now that we have a model capable of identifying aspects and mapping sentiments within each aspect-sentiment pair (to varying degrees of success). But how \"*good*\" is this model? To evaluate this, we now evaluate the model's F1 score. </br>\n",
        "\n",
        ">The **F1 score** is a common metric used to evaluate natural language processing (NLP) models that specialize in areas such as classification, extraction, and ABSA. It is the harmonic mean of the model's **precision** (how precise is this model predictions?) and **recall** (how many relevant things did it find?). In short, it measures the model’s ability to produce correct outputs while avoiding incorrect ones.\n",
        "\n",
        "The equation to calculate an F1 score is as follows:\n",
        "> $$ F1 = 2 × (\\frac{Precision × Recall}{Precision + Recall}) $$\n",
        "\n",
        "In the context of this model, there are two tasks being performed to be evaluated: **aspect extraction**, and **sentiment classification**."
      ],
      "metadata": {
        "id": "TQUtLl3rXyKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#parse absa text outputs into structured data for F1 scoring function to parse\n",
        "\n",
        "def absa_generate(text, max_length=128):\n",
        "\n",
        "    #instruction for output format\n",
        "    instruction = (\n",
        "        \"Extract aspect-based sentiment. \"\n",
        "        \"Return outputs in format: 'aspect=<term>, sentiment=<positive|negative|neutral>' \"\n",
        "        \"separated by '; ' or 'none' if no aspects.\\n\\n\"\n",
        "    )\n",
        "    #store input text as instruction + absa prefix + raw text\n",
        "    input_text = instruction + \"ABSA: \" + text\n",
        "    #tokenize string and convert to PyTorch tensors\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    #generate output text\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=max_length, num_beams=4, early_stopping=True)\n",
        "    #convert token ids back into raw text\n",
        "    raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return raw\n",
        "\n",
        "#convert ABSA results (like \"aspect=food, sentiment=positive; aspect=service, sentiment=negative\") back to format the dataset originally was in ({\"aspect\": \"food\", \"sentiment\": \"positive\"}, {\"aspect\": \"food\", \"sentiment\": \"positive\"},\n",
        "#{\"aspect\": \"service\", \"sentiment\": \"negative\"}\n",
        "\n",
        "def parse_aspect_string(s):\n",
        "    s = s.strip()\n",
        "    #if no predictions found\n",
        "    if not s or s.lower() in {\"none\", \"no aspects\", \"[]\"}:\n",
        "        return []\n",
        "    pairs = []\n",
        "    #split on ';' if multiple aspects found, then parse each part\n",
        "    for part in s.split(\";\"):\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        #match aspect=<term>, sentiment=<polarity>\n",
        "        m = re.search(r\"aspect\\s*=\\s*(.+?)\\s*,\\s*sentiment\\s*=\\s*(positive|negative|neutral)\", part, flags=re.I)\n",
        "        if m:\n",
        "            aspect = m.group(1).strip()\n",
        "            sentiment = m.group(2).strip().lower()\n",
        "            pairs.append({\"aspect\": aspect, \"sentiment\": sentiment})\n",
        "        else:\n",
        "            #fallback: try `X was Y` style (rare) or ignore\n",
        "            m2 = re.search(r\"(.+?)\\s+was\\s+(positive|negative|neutral)\", part, flags=re.I)\n",
        "            if m2:\n",
        "                pairs.append({\"aspect\": m2.group(1).strip(), \"sentiment\": m2.group(2).lower()})\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "duyKF5kwZ_eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find correct pairs, convert dataset example with `aspectTerms` into our tuple list form\n",
        "\n",
        "def extract_good_pairs(example):\n",
        "    pairs = []\n",
        "    for asp in example.get(\"aspectTerms\", []):\n",
        "        term = asp.get(\"term\")\n",
        "        pol = asp.get(\"polarity\")\n",
        "        if term and pol:\n",
        "            pairs.append({\"aspect\": term, \"sentiment\": pol.lower()})\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "mFCoPz7q5Zd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, output test to check if aspect strings converted properly back to original dataset format\n",
        "\n",
        "for s in [\n",
        "    \"The food was amazing but the service was terrible.\",\n",
        "    \"I like the ambiance, but the drinks are overpriced.\"\n",
        "]:\n",
        "    raw = absa_generate(s)\n",
        "    print(\"RAW:\", raw)\n",
        "    print(\"PARSED:\", parse_aspect_string(raw))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPa2rBu95GvP",
        "outputId": "587e5993-b7b3-4b88-a69f-a1bebe1e1238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW: aspect=food, sentiment=positive; aspect=service, sentiment=positive\n",
            "PARSED: [{'aspect': 'food', 'sentiment': 'positive'}, {'aspect': 'service', 'sentiment': 'positive'}]\n",
            "\n",
            "RAW: aspect=ambiance, sentiment=positive; aspect=drinks, sentiment=positive\n",
            "PARSED: [{'aspect': 'ambiance', 'sentiment': 'positive'}, {'aspect': 'drinks', 'sentiment': 'positive'}]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#computing joint f1 score of aspect extraction + sentiment classification\n",
        "\n",
        "def compute_f1(true_pairs, pred_pairs):\n",
        "\n",
        "    #convert list of dicts into tuples\n",
        "\n",
        "    #true_pairs: list of dicts [{\"aspect\":..., \"sentiment\":...}, ...]\n",
        "    true_set = set((p[\"aspect\"].lower(), p[\"sentiment\"].lower()) for p in true_pairs)\n",
        "    #pred_pairs: list of dicts of same form\n",
        "    pred_set = set((p[\"aspect\"].lower(), p[\"sentiment\"].lower()) for p in pred_pairs)\n",
        "\n",
        "    #true positive, actual and prediction = true, correct match\n",
        "    TP = len(true_set & pred_set)\n",
        "\n",
        "    #false positive, predicted true but not true, not correct\n",
        "    FP = len(pred_set - true_set)\n",
        "    #false negative, did not predict true on true, not correct\n",
        "    FN = len(true_set - pred_set)\n",
        "\n",
        "    #returns precision, recall, f1 for the joint match (aspect extraction + sentiment classification, refer to formula)\n",
        "    precision = TP / (TP + FP + 1e-12)  #small epsilon added to prevent /0 errors\n",
        "    recall = TP / (TP + FN + 1e-12)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-12) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "metadata": {
        "id": "REUF3q60aNCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to run on dataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_on_dataset(split_dataset, limit=None):\n",
        "    tot_p = tot_r = tot_f1 = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for i, ex in enumerate(tqdm(split_dataset)):  #loop through dataset\n",
        "        if limit and i >= limit:\n",
        "            break\n",
        "\n",
        "        #get the good labels from preprocess metadata\n",
        "        gold = extract_good_pairs({\n",
        "            \"aspectTerms\": ex[\"raw_aspects\"]\n",
        "        })\n",
        "\n",
        "        #build the input text\n",
        "        input_text = \"ABSA: \" + ex[\"raw_text\"]\n",
        "\n",
        "        #run model inference\n",
        "        raw_output = absa_generate(input_text)\n",
        "\n",
        "        #parse prediction into same structure as good pairs\n",
        "        pred = parse_aspect_string(raw_output)\n",
        "\n",
        "        #compute F1 for this sample\n",
        "        p, r, f1 = compute_f1(gold, pred)\n",
        "\n",
        "        tot_p += p\n",
        "        tot_r += r\n",
        "        tot_f1 += f1\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"precision\": tot_p / n,\n",
        "        \"recall\": tot_r / n,\n",
        "        \"f1\": tot_f1 / n\n",
        "    }"
      ],
      "metadata": {
        "id": "Va8t2dNRaSCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = evaluate_on_dataset(valid_dataset)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDfMMqSacPS",
        "outputId": "fd6fc8f6-03f6-49b5-85b5-d8fa0de9a531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [11:26<00:00,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.3432499999997486, 'recall': 0.3116577380950211, 'f1': 0.3207022283268132}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}