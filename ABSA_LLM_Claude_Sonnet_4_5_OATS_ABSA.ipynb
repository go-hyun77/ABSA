{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1A_m9uWnTdShb5XC3nhaST1LmfR8kSTb8",
      "authorship_tag": "ABX9TyP+fzviIST5aGRspiGQEs3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/go-hyun77/ABSA/blob/main/ABSA_LLM_Claude_Sonnet_4_5_OATS_ABSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Aspect-Based Sentiment Analysis (ABSA) with Claude-Sonnet 4.5</u>**\n",
        "This notebook implements a **Claude-Sonnet 4.5** based LLM capable of performing aspect-based sentiment analysis on the [OATS-ABSA dataset](https://huggingface.co/datasets/jordiclive/OATS-ABSA). Instructions/explanations for testing/reviewing each code block will be outlined for posterity purposes.\n",
        "\n",
        "> To recap, the previous **[T5](https://huggingface.co/docs/transformers/en/model_doc/t5) (Text-to-Text Transfer Transformer)** model implemented followed a standard machine learning pipeline of:\n",
        "> 1.   Load base T5-small model\n",
        "  2.   Fine tune on labelled, preprocessed data\n",
        "  3.   Generate predictions after training\n",
        "\n",
        "Observations on the output results of the previous T5 implementation indicate that aspect extraction, sentiment classification **and** most notably aspect-to-sentiment mapping for T5 is incredibly fragile. T5 has been observed to:\n",
        "*   Fail to extract multiple aspects within a sentence\n",
        "*   Perform incomplete extraction of sentiments\n",
        "*   Hallucinate/misspell aspects\n",
        "*   Fail to identify implicit aspects\n",
        "*   Fail to correctly match sentiment to aspect\n",
        "*   Fail to extract aspects due to formatting issues\n",
        "*   Output minor formatting errors lowering F1 calculation\n",
        "\n",
        "With Sonnet 4.5, we aim to make use of its massive pre-trained corpora to perform aspect extraction, sentiment classification and mapping, all while eliminating the need for training. For this implementation, we focus on zero-shot/few-shot prompting to perform ABSA.\n",
        "\n",
        "The expected input/outputs of this model will be as follows in the given example:\n",
        "```\n",
        "INPUT: to be outlined\n",
        "```\n",
        "```\n",
        "OUTPUT: to be outlined\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bOoFUSJbIR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies and import libraries\n",
        "!pip install anthropic datasets pandas scikit-learn tqdm\n",
        "\n",
        "import anthropic\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive') #mount drive for saving/loading model\n",
        "model_dir = \"/content/drive/MyDrive/ABSA_Sonnet4_Model\" #define model directory in google drive, you may need to modify this link to point to the appropriate directory\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "#get API key from colab secrets\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "\n",
        "#initialize client\n",
        "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "#test the connection\n",
        "print(\"Testing API connection...\")\n",
        "try:\n",
        "    test_message = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=10,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
        "    )\n",
        "    print(\"API key is valid. Connection successful.\")\n",
        "except Exception as e:\n",
        "    print(f\" API key test failed: {e}\")\n",
        "\n",
        "\n",
        "#configure anthropic api key in colab secrets\n",
        "ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', 'ANTHROPIC_API_KEY')\n",
        "\n",
        "#initialize the client with the API key from secrets\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "#identify claude model (Sonnet 4.5 for latest)\n",
        "MODEL_NAME = \"claude-sonnet-4-20250514\"\n",
        "\n",
        "#initialize anthropic client\n",
        "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "print(f\"Claude model: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "L5d8yE95AOwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553b3567-157b-43bf-ac63-0557f605a7fa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.75.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Testing API connection...\n",
            "API key is valid. Connection successful.\n",
            "Claude model: claude-sonnet-4-20250514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check, test if the secret exists and is loaded\n",
        "try:\n",
        "    test_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "    print(f\"API Key loaded: {test_key[:10]}...{test_key[-4:]}\")  # Show first 10 and last 4 chars only\n",
        "    print(f\"Key length: {len(test_key)} characters\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading API key: {e}\")\n",
        "    print(\"\\nMake sure you:\")\n",
        "    print(\"1. Created a secret named exactly 'ANTHROPIC_API_KEY' (case-sensitive)\")\n",
        "    print(\"2. Toggled 'Notebook access' to ON\")\n",
        "    print(\"3. Your API key starts with 'sk-ant-api03-'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo2c-8XgUTPy",
        "outputId": "ce20f377-364f-437a-f4bf-a5cadded5808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded: sk-ant-api...pQAA\n",
            "Key length: 108 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Loading and Examining the Dataset</u>**\n",
        "In this notebook, we will load and work with the [OATS-ABSA dataset](https://huggingface.co/datasets/jordiclive/OATS-ABSA) with the following line:\n",
        "```\n",
        "dataset = load_dataset(\"alexcadillon/SemEval2014Task4\", \"restaurants\")\n",
        "```\n",
        "The [OATS-ABSA dataset](https://huggingface.co/datasets/jordiclive/OATS-ABSA)'s data columns contain the following attributes outlined in the below table:\n",
        "\n",
        "| Field Name | Data Type | Description |\n",
        "| :------- | :------: | -------: |\n",
        "| comment  | string  | The actual raw text content of the sentence.  |\n",
        "| quad  | list | Ground truth aspect-sentiment pairs in the format of: [aspect_category, sentiment]  |\n",
        "| dataset | string | Domain identifier (hotels, amazon_ff, or coursera).  |\n"
      ],
      "metadata": {
        "id": "rO5g4-yZggqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "dataset = load_dataset(\"jordiclive/OATS-ABSA\")\n",
        "\n",
        "#examine dataset structure\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvwF_kT0f83l",
        "outputId": "98975d83-9b9e-4736-c29e-a92f1b47a9d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['comment', 'quad', 'dataset'],\n",
            "        num_rows: 3987\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['comment', 'quad', 'dataset'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['comment', 'quad', 'dataset'],\n",
            "        num_rows: 170\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#examine first few examples\n",
        "for i in range(3):\n",
        "    example = dataset['train'][i]\n",
        "    print(f\"\\n{i+1}. comment: {example['comment'][:100]}...\")\n",
        "    print(f\"   quad: {example['quad']}\")\n",
        "    print(f\"   dataset: {example['dataset']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enX8LKwIij_D",
        "outputId": "1e080361-0cd3-44a4-af2c-7c2f1b8650bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. comment: Fantastic service I am a travel agent booking hotels all over the world, so we are very very fussy. ...\n",
            "   quad: [['service general', 'positive'], ['location general', 'positive'], ['rooms general', 'positive'], ['rooms prices', 'positive'], ['hotel general', 'positive']]\n",
            "   dataset: hotels\n",
            "\n",
            "2. comment: venient, helpful  I stayed here as a single female and it was a great place to be. The concierge was...\n",
            "   quad: [['hotel general', 'positive'], ['service general', 'positive'], ['location general', 'positive']]\n",
            "   dataset: hotels\n",
            "\n",
            "3. comment: Always a comfortable stay Small rooms but nice hotel. Just had a restaurant and bar makeover - much ...\n",
            "   quad: [['rooms comfort', 'positive'], ['rooms design_features', 'negative'], ['hotel general', 'positive'], ['food_drinks quality', 'positive'], ['hotel miscellaneous', 'positive'], ['service general', 'positive'], ['location general', 'positive']]\n",
            "   dataset: hotels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#examine train and test splits\n",
        "\n",
        "#get unique aspect categories and sentiments\n",
        "def analyze_dataset_structure(dataset_split, name=\"train\"):\n",
        "    aspect_categories = set()\n",
        "    sentiments = set()\n",
        "    domains = set()\n",
        "\n",
        "    for example in dataset_split:\n",
        "        domains.add(example['dataset'])\n",
        "        for quad in example['quad']:\n",
        "            aspect_categories.add(quad[0])\n",
        "            sentiments.add(quad[1])\n",
        "\n",
        "    print(f\"\\n{name.upper()} SET STATISTICS:\")\n",
        "    print(f\"Total examples: {len(dataset_split)}\")\n",
        "    print(f\"Domains: {sorted(domains)}\")\n",
        "    print(f\"Unique aspect categories: {len(aspect_categories)}\")\n",
        "    print(f\"Sample categories: {sorted(list(aspect_categories))[:10]}\")\n",
        "    print(f\"Sentiments: {sorted(sentiments)}\")\n",
        "\n",
        "    return aspect_categories, sentiments\n",
        "\n",
        "train_aspects, train_sentiments = analyze_dataset_structure(dataset['train'], \"train\")\n",
        "test_aspects, test_sentiments = analyze_dataset_structure(dataset['test'], \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rnSeKh5MCcT",
        "outputId": "5b9f7b95-5c9c-4f98-a052-c551b33b5e73"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAIN SET STATISTICS:\n",
            "Total examples: 3987\n",
            "Domains: ['amazon_ff', 'coursera', 'hotels']\n",
            "Unique aspect categories: 74\n",
            "Sample categories: ['amazon availability', 'amazon prices', 'assignments comprehensiveness', 'assignments quality', 'assignments quantity', 'assignments relatability', 'assignments workload', 'course comprehensiveness', 'course general', 'course quality']\n",
            "Sentiments: ['conflict', 'negative', 'neutral', 'positive']\n",
            "\n",
            "TEST SET STATISTICS:\n",
            "Total examples: 500\n",
            "Domains: ['amazon_ff', 'coursera', 'hotels']\n",
            "Unique aspect categories: 64\n",
            "Sample categories: ['amazon availability', 'amazon prices', 'assignments comprehensiveness', 'assignments quality', 'assignments quantity', 'assignments relatability', 'assignments workload', 'course comprehensiveness', 'course general', 'course quality']\n",
            "Sentiments: ['conflict', 'negative', 'neutral', 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Prompt Engineering</u>**\n",
        "[Role prompting](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/system-prompts) is used to enhance Claude by specializing the model into a specific domain (in this context of this project, being an ABSA \"expert\"). This process can be broken down into two parts, \"system\" and \"user\" prompting.\n",
        "\n",
        "<br>\n",
        "\n",
        "System prompting involves sending Claude \"foundational\"  instructions that define its overarching \"purpose\", essentially specializing the model into a certain domain by defining the following in the context of the model:\n",
        "* Role Specialization: Defining the model's role and expertise.\n",
        "* Constraints and Limitations: Establishing rules for the AI's responses, such as outputting text in a certain format.\n",
        "* Context Definition: Providing situational context to the input to the model.\n",
        "\n",
        "User prompting includes the specific instructions, questions, and/or text input a user provides to the model to elicit a desired response.\n",
        ">In the context of this model, we provide examples of correct aspect extraction, sentiment classification, and sentiment-to-aspect mapping, while also hightlighting the desired output format."
      ],
      "metadata": {
        "id": "KgdmjdRTkCYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define system prompt (pinned, overarching instructions)\n",
        "\n",
        "def create_system_prompt(aspect_categories: List[str] = None) -> str:\n",
        "\n",
        "    base_prompt = \"\"\"You are an expert at Aspect-Based Sentiment Analysis (ABSA) for reviews.\n",
        "\n",
        "    Your task is to analyze reviews and extract aspect-sentiment pairs at the CATEGORY level (not just specific terms).\n",
        "\n",
        "    IMPORTANT DISTINCTIONS:\n",
        "    - Aspect CATEGORIES are conceptual (e.g., \"service general\", \"rooms comfort\", \"food quality\")\n",
        "    - They may not appear explicitly in the text\n",
        "    - You must infer the category from context\n",
        "\n",
        "    OUTPUT FORMAT:\n",
        "    Return ONLY a JSON array with this exact structure:\n",
        "    [\n",
        "      {\n",
        "        \"aspect_category\": \"category_subcategory\",\n",
        "        \"sentiment\": \"positive\" or \"negative\" or \"neutral\" or \"conflict\"\n",
        "      }\n",
        "    ]\n",
        "\n",
        "    SENTIMENT DEFINITIONS:\n",
        "    - positive: Favorable opinion\n",
        "    - negative: Unfavorable opinion\n",
        "    - neutral: Factual statement without clear sentiment\n",
        "    - conflict: Mixed sentiments in same review about same aspect\n",
        "\n",
        "    RULES:\n",
        "    1. Extract ALL aspect categories mentioned (explicit or implicit)\n",
        "    2. Use lowercase for aspect categories with underscore (e.g., \"rooms_design_features\")\n",
        "    3. If no aspects found, return empty array []\n",
        "    4. Return ONLY the JSON array, no other text\"\"\"\n",
        "\n",
        "    if aspect_categories:\n",
        "        #add known categories to help the model\n",
        "        categories_text = \", \".join(sorted(aspect_categories)[:50])  # Show first 50\n",
        "        base_prompt += f\"\\n\\nCOMMON ASPECT CATEGORIES IN THIS DOMAIN:\\n{categories_text}\"\n",
        "\n",
        "    return base_prompt"
      ],
      "metadata": {
        "id": "gvFGvHGMPyvo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define user prompt, show the model some examples if few-shot\n",
        "\n",
        "def create_user_prompt(text: str, few_shot_examples: List[Dict] = None) -> str:\n",
        "\n",
        "    prompt = \"\"\n",
        "\n",
        "    #add fewshot examples if provided\n",
        "    if few_shot_examples:\n",
        "        prompt += \"Here are some examples:\\n\\n\"\n",
        "        for i, ex in enumerate(few_shot_examples[:3], 1):  #3 examples max\n",
        "            prompt += f\"Example {i}:\\n\"\n",
        "            prompt += f\"Text: \\\"{ex['text']}\\\"\\n\"\n",
        "            prompt += f\"Output: {json.dumps(ex['quads'], indent=2)}\\n\\n\"\n",
        "\n",
        "    #add the actual text to analyze\n",
        "    prompt += f\"Now analyze this review:\\n\\nText: \\\"{text}\\\"\\n\\nOutput:\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "czU4tgDWw3Ef"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#claude inference to predict aspect-sentiment quads (this would have been the T5 model.generate() funciton)\n",
        "\n",
        "def predict_with_claude(\n",
        "    #define parameters\n",
        "    text: str,  #review text to analyze\n",
        "    system_prompt: str,  #system prompt for model defined above\n",
        "    few_shot_examples: List[Dict] = None, #optional exmaples to show model\n",
        "    max_retries: int = 3, #num of retries if failed API call\n",
        "    temperature: float = 0.0  #randomness of output, 0->1 strict to flexible\n",
        ") -> List[Dict]:  #return list of dicts with aspect_category and sentiment\n",
        "\n",
        "    user_prompt = create_user_prompt(text, few_shot_examples)\n",
        "\n",
        "    #error handling loop\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            message = client.messages.create( #claude api call\n",
        "                model=MODEL_NAME, #sonnet4.5\n",
        "                max_tokens=2048,  #max length of response ~1500 words\n",
        "                temperature=temperature,  #0 for deterministic response\n",
        "                system=system_prompt, #above defined instructions\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}  #user prompt from above\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            response_text = message.content[0].text.strip() #parse text from claude response and remove whitespace\n",
        "\n",
        "            #clean response, remove markdown if present\n",
        "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
        "            #parse json, convert json string to list of dicts\n",
        "            predictions = json.loads(response_text)\n",
        "            #validate structure, check for list structure\n",
        "            if not isinstance(predictions, list):\n",
        "                raise ValueError(\"Response is not a list.\")\n",
        "\n",
        "            #normalize format\n",
        "            normalized = []\n",
        "\n",
        "            for pred in predictions:\n",
        "                #check for aspect_category and sentiment in response\n",
        "                if isinstance(pred, dict) and 'aspect_category' in pred and 'sentiment' in pred:\n",
        "                    normalized.append({\n",
        "                        #convert to lowercase and remove whitespace to match ground truth formatting\n",
        "                        'aspect_category': pred['aspect_category'].lower().strip(),\n",
        "                        'sentiment': pred['sentiment'].lower().strip()\n",
        "                    })\n",
        "            return normalized\n",
        "\n",
        "        #error handling, if failed json parse\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON decode error (attempt {attempt + 1}): {e}\")\n",
        "            print(f\"Response: {response_text[:200]}\") #output generated response for debug\n",
        "            if attempt == max_retries - 1:  #retry every 1 second if not last attempt\n",
        "                return []\n",
        "            time.sleep(1)\n",
        "\n",
        "        #error handling, other errors\n",
        "        except Exception as e:\n",
        "            print(f\"API error (attempt {attempt + 1}): {e}\")\n",
        "            if attempt == max_retries - 1:  #retry every 2 seconds if not last attempt\n",
        "                return []\n",
        "            time.sleep(2)\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "QDc0QPc-P5MS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Input Processing</u>**\n",
        "The dataset input is combed to remove whitespace and uppercase letters, while converting the original format of nested lists to a list of dictionaries.\n",
        ">Input processing of the dataset is necessary to match the output of the model, ensuring a correct match when calcuating the F1 score (if formatting is off, it will count as a miss even if the predictions are semantically correct, as seen from our previous T5 model)."
      ],
      "metadata": {
        "id": "bISPS39RRpz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#format dataset input to list of dicts\n",
        "\n",
        "def format_ground_truth(example: Dict) -> List[Dict]:\n",
        "\n",
        "    #convert OATS format to list of dicts format\n",
        "    #original: [[\"service general\", \"positive\"], ...]\n",
        "    #formatted: [{\"aspect_category\": \"service general\", \"sentiment\": \"positive\"}, ...]\n",
        "\n",
        "    formatted = []\n",
        "    for quad in example['quad']:\n",
        "        formatted.append({\n",
        "            'aspect_category': quad[0].lower().strip(), #identify aspect category in quad\n",
        "            'sentiment': quad[1].lower().strip()  #identify sentiment in quad\n",
        "        })\n",
        "    return formatted"
      ],
      "metadata": {
        "id": "zsUJxbVFQCYj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Few-shot Examples</u>**\n",
        "Below we prepare examples of correct aspect extractions and sentiment classifications to feed to the model to give better context for predictions.\n",
        ">If testing the model for zero-shot, there will be a variable to toggle whether zero or few-shot is used in the \"Claude Evaluation on OATS-ABSA\" section. The output of this block will be used when the prompt is generated for Claude."
      ],
      "metadata": {
        "id": "3mfPZPeueQxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function create few-shot examples\n",
        "\n",
        "def prepare_few_shot_examples(dataset_split, n_examples: int = 5) -> List[Dict]:\n",
        "\n",
        "    examples = []\n",
        "\n",
        "    for i in range(min(n_examples, len(dataset_split))):  #loop and extract sample entries from dataset, prevent out of range with min\n",
        "        ex = dataset_split[i]\n",
        "        examples.append({ #append parsed text and formatted quads to examples\n",
        "            'text': ex['comment'],\n",
        "            'quads': format_ground_truth(ex)\n",
        "        })\n",
        "    return examples\n",
        "\n",
        "#extract 5 examples from the dataset training split\n",
        "few_shot_examples = prepare_few_shot_examples(dataset['train'], n_examples=5)\n",
        "\n",
        "print(f\"Extracted {len(few_shot_examples)} samples for few-shot\")\n",
        "\n",
        "#sanity check, output extracted samples to verify correct format\n",
        "print(\"\\nExtracted few-shot format:\")\n",
        "print(json.dumps(few_shot_examples[0], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYbZcarEeP-E",
        "outputId": "02de27ae-a5dc-483a-ade7-5f3f871238de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5 samples for few-shot\n",
            "\n",
            "Extracted few-shot format:\n",
            "{\n",
            "  \"text\": \"Fantastic service I am a travel agent booking hotels all over the world, so we are very very fussy. This hotel offers customer service at its' best. Great location, great service, nothing is too much bother. The rooms are great, cheaper than some of the well known names and frankly far better. Stay here and give yourself a treat with staff who really do genuinely care. We will continue to book our clients here, as it gives a great satisfaction to offer them such an outstanding property.\",\n",
            "  \"quads\": [\n",
            "    {\n",
            "      \"aspect_category\": \"service general\",\n",
            "      \"sentiment\": \"positive\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect_category\": \"location general\",\n",
            "      \"sentiment\": \"positive\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect_category\": \"rooms general\",\n",
            "      \"sentiment\": \"positive\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect_category\": \"rooms prices\",\n",
            "      \"sentiment\": \"positive\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect_category\": \"hotel general\",\n",
            "      \"sentiment\": \"positive\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>F1 Score Evaluation Functions</u>**\n",
        "Below are the functions to calculate the model's F1 score. </br>\n",
        "\n",
        ">The **F1 score** is a common metric used to evaluate natural language processing (NLP) models that specialize in areas such as classification, extraction, and ABSA. It is the harmonic mean of the model's **precision** (how precise is this model predictions?) and **recall** (how many relevant things did it find?). In short, it measures the model’s ability to produce correct outputs while avoiding incorrect ones.\n",
        "\n",
        "The equation to calculate an F1 score is as follows:\n",
        "> $$ F1 = 2 × (\\frac{Precision × Recall}{Precision + Recall}) $$\n",
        "\n",
        "Similar to the previous T5 model, there are two tasks being performed to be evaluated: **aspect detection**, and **sentiment classification**."
      ],
      "metadata": {
        "id": "EzWNljodjJmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#f1 score evaluation, calculate aspect extraction and aspect-sentiment classification\n",
        "\n",
        "def compute_f1(predictions: List[Dict], ground_truth: List[Dict]) -> Dict:\n",
        "\n",
        "    #convert list of dicts into tuples (set) to compare both aspect + sentiment together as a pair\n",
        "\n",
        "    #true pairs\n",
        "    true_set = set((g['aspect_category'], g['sentiment']) for g in ground_truth)\n",
        "    #predicted pairs\n",
        "    pred_set = set((p['aspect_category'], p['sentiment']) for p in predictions)\n",
        "\n",
        "\n",
        "    #aspect only for aspect extraction accuracy\n",
        "\n",
        "    #true aspects\n",
        "    true_aspects = set(g['aspect_category'] for g in ground_truth)\n",
        "    #predicted aspects\n",
        "    pred_aspects = set(p['aspect_category'] for p in predictions)\n",
        "\n",
        "\n",
        "    #joint metrics (aspect + sentiment must both match)\n",
        "\n",
        "    #true positive, actual and prediction = true, correct match\n",
        "    TP_joint = len(true_set & pred_set)\n",
        "    #false positive, predicted true but not true, not correct\n",
        "    FP_joint = len(pred_set - true_set)\n",
        "    #false negative, did not predict true on true, not correct\n",
        "    FN_joint = len(true_set - pred_set)\n",
        "\n",
        "    #aspect metrics (just aspect extraction)\n",
        "\n",
        "    #true positive, actual and prediction = true, correct match\n",
        "    TP_aspect = len(pred_aspects & true_aspects)\n",
        "    #false positive, predicted true but not true, not correct\n",
        "    FP_aspect = len(pred_aspects - true_aspects)\n",
        "    #false negative, did not predict true on true, not correct\n",
        "    FN_aspect = len(true_aspects - pred_aspects)\n",
        "\n",
        "\n",
        "    #F1 calculation\n",
        "\n",
        "    #division function to prevent divide by 0\n",
        "    def safe_div(a, b):\n",
        "        return a / b if b > 0 else 0.0\n",
        "\n",
        "    #joint F1 (aspect and sentiment must match)\n",
        "    precision_joint = safe_div(TP_joint, TP_joint + FP_joint)\n",
        "    recall_joint = safe_div(TP_joint, TP_joint + FN_joint)\n",
        "    F1_joint = safe_div(2 * precision_joint * recall_joint, precision_joint + recall_joint)\n",
        "\n",
        "    #aspect only F1 (just aspect extraction)\n",
        "    precision_aspect = safe_div(TP_aspect, TP_aspect + FP_aspect)\n",
        "    recall_aspect = safe_div(TP_aspect, TP_aspect + FN_aspect)\n",
        "    F1_aspect = safe_div(2 * precision_aspect * recall_aspect, precision_aspect + recall_aspect)\n",
        "\n",
        "    return {\n",
        "        #joint metrics (aspect + sentiment)\n",
        "        'precision_joint': precision_joint, #of all predictions made, how many correct\n",
        "        'recall_joint': recall_joint, #of all ground truth, how many did we detect\n",
        "        'F1_joint': F1_joint,\n",
        "        'TP_joint': TP_joint,\n",
        "        'FP_joint': FP_joint,\n",
        "        'FN_joint': FN_joint,\n",
        "        #aspect-only metrics\n",
        "        'precision_aspect': precision_aspect,\n",
        "        'recall_aspect': recall_aspect,\n",
        "        'F1_aspect': F1_aspect,\n",
        "        'TP_aspect': TP_aspect,\n",
        "        'FP_aspect': FP_aspect,\n",
        "        'FN_aspect': FN_aspect,\n",
        "    }"
      ],
      "metadata": {
        "id": "34qQ7NnoQPoi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Claude Evaluation on OATS-ABSA</u>**\n",
        "Below we evaluate Claude on the [OATS-ABSA dataset](https://huggingface.co/datasets/jordiclive/OATS-ABSA). Relative to the previous T5 model, the below block is the equivalent of executing the trainer.train() function to introduce the dataset to the T5 model. Various parameters to control aspects of prediction generation are listed below, ranging from which dataset split (test or train) to evaluate on, to feeding few-shot examples defined earlier for improved accuracy."
      ],
      "metadata": {
        "id": "XdTmoxAaY7sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define evaluation parameters\n",
        "\n",
        "def evaluate_claude_on_dataset(\n",
        "    dataset_split,  #which part to evaluate on (train or test splits)\n",
        "    system_prompt: str, #system prompt instructions defined in Prompt Engineering section\n",
        "    num_samples: int = None,  #number of samples to evaluate, if none then all, number for quick test\n",
        "    use_few_shot: bool = True,  #true for few-shot, false for zero-shot\n",
        "    few_shot_examples: List[Dict] = None, #examples defined in Prompt Engineering section\n",
        "    save_results: bool = True,  #save toggle\n",
        "    results_file: str = \"claude_absa_results.json\"  #output file\n",
        "):\n",
        "\n",
        "    #if num_samples from above args is defined, then limit amount of dataset to first num of dataset\n",
        "    if num_samples:\n",
        "        dataset_split = dataset_split.select(range(min(num_samples, len(dataset_split))))\n",
        "\n",
        "    #init lists\n",
        "    results = []\n",
        "    all_metrics = []\n",
        "\n",
        "    #pass in few-shot examples from above block if they exist\n",
        "    few_shot = few_shot_examples if use_few_shot else None\n",
        "\n",
        "    print(f\"\\nEvaluating {len(dataset_split)} examples...\")\n",
        "    print(f\"Few-shot learning: {'ENABLED' if use_few_shot else 'DISABLED'}\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "    #evaluation loop, for each example in the dataset\n",
        "    for idx, example in enumerate(tqdm(dataset_split, desc=\"Evaluating\")):\n",
        "\n",
        "        #get ground truth and format to list of dicts\n",
        "        ground_truth = format_ground_truth(example)\n",
        "        text = example['comment']\n",
        "\n",
        "        #get claude predictions with inference function\n",
        "        predictions = predict_with_claude(\n",
        "            text=text,\n",
        "            system_prompt=system_prompt,\n",
        "            few_shot_examples=few_shot\n",
        "        )\n",
        "\n",
        "        #compute metrics for this example\n",
        "        metrics = compute_f1(predictions, ground_truth)\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        #store results from this parsed/predicted example\n",
        "        result = {\n",
        "            'idx': idx,\n",
        "            'text': text,\n",
        "            'domain': example['dataset'],\n",
        "            'ground_truth': ground_truth,\n",
        "            'predictions': predictions,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "        results.append(result)  #store in results list\n",
        "\n",
        "        #print progress every 50 examples\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            avg_f1 = sum(m['F1_joint'] for m in all_metrics) / len(all_metrics)\n",
        "            print(f\"\\nProgress: {idx + 1}/{len(dataset_split)} | Avg F1 (joint): {avg_f1:.3f}\")\n",
        "\n",
        "    #aggregate metrics for joint and aspect only F1s\n",
        "    n = len(all_metrics)\n",
        "    aggregated = {\n",
        "        'precision_joint': sum(m['precision_joint'] for m in all_metrics) / n,\n",
        "        'recall_joint': sum(m['recall_joint'] for m in all_metrics) / n,\n",
        "        'F1_joint': sum(m['F1_joint'] for m in all_metrics) / n,\n",
        "        'precision_aspect': sum(m['precision_aspect'] for m in all_metrics) / n,\n",
        "        'recall_aspect': sum(m['recall_aspect'] for m in all_metrics) / n,\n",
        "        'F1_aspect': sum(m['F1_aspect'] for m in all_metrics) / n,\n",
        "    }\n",
        "\n",
        "    #save detailed results\n",
        "    if save_results:\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'aggregated_metrics': aggregated,\n",
        "                'detailed_results': results,\n",
        "                'config': {\n",
        "                    'model': MODEL_NAME,\n",
        "                    'num_samples': len(dataset_split),\n",
        "                    'use_few_shot': use_few_shot,\n",
        "                    'num_few_shot_examples': len(few_shot) if few_shot else 0\n",
        "                }\n",
        "            }, f, indent=2)\n",
        "        print(f\"\\nDetailed results saved to: {results_file}\")\n",
        "\n",
        "    return aggregated, results"
      ],
      "metadata": {
        "id": "Dn6qXVKBQSa1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check, test anthropic api key is correctly init\n",
        "\n",
        "#re-initialize the client with the API key from colab secrets\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "print(\"Client re-initialized with API key from Colab Secrets.\")\n",
        "\n",
        "#test to verify it works\n",
        "try:\n",
        "    test_message = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=10,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
        "    )\n",
        "    print(\"API connection test successful.\")\n",
        "    print(f\"Response: {test_message.content[0].text}\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection test failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXVR6dVTVN72",
        "outputId": "f1b1a9c3-a18f-4084-f0a9-f09c4bb8bc1e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client re-initialized with API key from Colab Secrets.\n",
            "API connection test successful.\n",
            "Response: Hello! How are you doing today? Is there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run evaluation function on subset for working verification\n",
        "\n",
        "#call system prompt function to define system prompt for this run\n",
        "system_prompt = create_system_prompt(aspect_categories=list(train_aspects))\n",
        "\n",
        "#test on a small subset first (10 examples)\n",
        "print(\"\\n>>> TESTING ON 10 EXAMPLES (Quick validation)\")\n",
        "test_metrics, test_results = evaluate_claude_on_dataset(\n",
        "    dataset_split=dataset['test'],\n",
        "    system_prompt=system_prompt,\n",
        "    num_samples=10,\n",
        "    use_few_shot=True,\n",
        "    few_shot_examples=few_shot_examples,\n",
        "    save_results=False\n",
        ")\n",
        "\n",
        "print(\"Test Results (10 examples)\")\n",
        "print(\"-\"*60)\n",
        "print(f\"Joint F1 (Aspect + Sentiment): {test_metrics['F1_joint']:.3f}\")\n",
        "print(f\"Aspect-Only F1 (Category Detection): {test_metrics['F1_aspect']:.3f}\")\n",
        "print(f\"Joint Precision: {test_metrics['precision_joint']:.3f}\")\n",
        "print(f\"Joint Recall: {test_metrics['recall_joint']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJbvDEKDo8wD",
        "outputId": "5356722b-83be-4de2-ae71-b7fdfe19a789"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> TESTING ON 10 EXAMPLES (Quick validation)\n",
            "\n",
            "Evaluating 10 examples...\n",
            "Few-shot learning: ENABLED\n",
            "Model: claude-sonnet-4-20250514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 10/10 [00:33<00:00,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results (10 examples)\n",
            "------------------------------------------------------------\n",
            "Joint F1 (Aspect + Sentiment): 0.658\n",
            "Aspect-Only F1 (Category Detection): 0.718\n",
            "Joint Precision: 0.642\n",
            "Joint Recall: 0.678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run evaluation function on full dataset\n",
        "\n",
        "#full evaluation\n",
        "print(\"\\n>>> RUNNING FULL EVALUATION\")\n",
        "full_metrics, full_results = evaluate_claude_on_dataset(\n",
        "     dataset_split=dataset['test'],\n",
        "     system_prompt=system_prompt,\n",
        "     num_samples=None,  #use all test examples\n",
        "     use_few_shot=True,\n",
        "     few_shot_examples=few_shot_examples,\n",
        "     save_results=True,\n",
        "     results_file=\"claude_oats_full_results.json\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh3mqGhxQWwj",
        "outputId": "a3b96cb6-ddaf-4ff9-8216-03380ee6e965"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> RUNNING FULL EVALUATION\n",
            "\n",
            "Evaluating 500 examples...\n",
            "Few-shot learning: ENABLED\n",
            "Model: claude-sonnet-4-20250514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  10%|█         | 50/500 [03:01<25:52,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 50/500 | Avg F1 (joint): 0.669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  20%|██        | 100/500 [06:10<26:05,  3.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 100/500 | Avg F1 (joint): 0.660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  30%|███       | 150/500 [09:30<24:48,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 150/500 | Avg F1 (joint): 0.648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  40%|████      | 200/500 [12:06<14:00,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 200/500 | Avg F1 (joint): 0.581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  50%|█████     | 250/500 [14:39<12:44,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 250/500 | Avg F1 (joint): 0.520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  60%|██████    | 300/500 [17:12<09:30,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 300/500 | Avg F1 (joint): 0.482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  70%|███████   | 350/500 [19:55<08:59,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 350/500 | Avg F1 (joint): 0.477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  80%|████████  | 400/500 [22:51<05:22,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 400/500 | Avg F1 (joint): 0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  90%|█████████ | 450/500 [26:18<02:59,  3.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 450/500 | Avg F1 (joint): 0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 500/500 [29:18<00:00,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Progress: 500/500 | Avg F1 (joint): 0.482\n",
            "\n",
            "Detailed results saved to: claude_oats_full_results.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Saving and Loading Results</u>**\n",
        "Below are code blocks to save and load the .json file containing results from the above run from Google Drive. </br>"
      ],
      "metadata": {
        "id": "rAXzcrTCrOMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save results to google drive for loading in subsequent sessions\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "#mount drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#create directory in drive\n",
        "DRIVE_RESULTS_PATH = '/content/drive/MyDrive/ABSA_Sonnet4_Model/'\n",
        "os.makedirs(DRIVE_RESULTS_PATH, exist_ok=True)\n",
        "print(f\"Directory created/verified: {DRIVE_RESULTS_PATH}\")\n",
        "\n",
        "#copy the local results file to drive\n",
        "local_file = \"claude_oats_full_results.json\"\n",
        "drive_file = os.path.join(DRIVE_RESULTS_PATH, \"claude_oats_full_results.json\")\n",
        "\n",
        "if os.path.exists(local_file):\n",
        "    shutil.copy(local_file, drive_file)\n",
        "    print(f\"Copied {local_file} to Google Drive\")\n",
        "    print(f\"Location: {drive_file}\")\n",
        "else:\n",
        "    print(f\"Could not find Colab local file: {local_file}\")  #if local file doesn't exist in colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqZnJbCgVReB",
        "outputId": "3751daa5-eb61-4620-eb08-bbb8ef47e0e6",
        "collapsed": true
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directory created/verified: /content/drive/MyDrive/ABSA_Sonnet4_Model/\n",
            "Copied claude_oats_full_results.json to Google Drive\n",
            "Location: /content/drive/MyDrive/ABSA_Sonnet4_Model/claude_oats_full_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load file from drive\n",
        "def load_results_from_drive(filename=\"claude_oats_results.json\"):\n",
        "\n",
        "    #load file from google drive\n",
        "    filepath = os.path.join(DRIVE_RESULTS_PATH, filename)\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Results loaded from: {filepath}\")\n",
        "        return data['aggregated_metrics'], data['detailed_results'], data['config']\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "P3gsErwBD4e-"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Results Analysis</u>**\n",
        "After loading the saved results, we examine the results of Claude's predictions on the [OATS-ABSA dataset](https://huggingface.co/datasets/jordiclive/OATS-ABSA)."
      ],
      "metadata": {
        "id": "e_T4EfviuJE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading saved results from drive\n",
        "\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "\n",
        "#set filepath as google drive location\n",
        "results_filepath = drive_file\n",
        "\n",
        "#load saved results from google drive\n",
        "try:\n",
        "    with open(results_filepath, 'r') as f:\n",
        "        saved_data = json.load(f)\n",
        "    print(f\"Results loaded from: {results_filepath}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {results_filepath}\")\n",
        "    #re-raise to stop execution if the file isn't found even after checking drive\n",
        "    raise\n",
        "\n",
        "#print num of results and F1 to check for correct file\n",
        "detailed_results = saved_data['detailed_results']\n",
        "aggregated_metrics = saved_data['aggregated_metrics']\n",
        "\n",
        "print(f\"Loaded {len(detailed_results)} results\")\n",
        "print(f\"Overall F1: {aggregated_metrics['F1_joint']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR5hpDL_6s7O",
        "outputId": "6187c8cc-555a-4f73-bcde-299982c61055"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results loaded from: /content/drive/MyDrive/ABSA_Sonnet4_Model/claude_oats_full_results.json\n",
            "Loaded 500 results\n",
            "Overall F1: 0.482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance analysis function\n",
        "\n",
        "def analyze_errors(results):\n",
        "\n",
        "    #track domains\n",
        "    domain_metrics = defaultdict(list)\n",
        "    domain_examples = defaultdict(int)\n",
        "\n",
        "    #track aspect categories\n",
        "    pred_aspects = Counter()\n",
        "    true_aspects = Counter()\n",
        "    missed_aspects = Counter()\n",
        "    false_aspects = Counter()\n",
        "\n",
        "    #track by position (to see if performance degrades over time)\n",
        "    position_f1 = []\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        domain = result['domain']\n",
        "        metrics = result['metrics']\n",
        "\n",
        "        #track metrics by domain\n",
        "        domain_metrics[domain].append(metrics['F1_joint'])\n",
        "        domain_examples[domain] += 1\n",
        "\n",
        "        #track F1 over position\n",
        "        position_f1.append((i, metrics['F1_joint']))\n",
        "\n",
        "    print(\"Performance Analysis\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "\n",
        "    #domain-wise performance between coursera, amazonff, and hotels\n",
        "    print(\"\\nF1 Score by Domain:\")\n",
        "\n",
        "    for domain in sorted(domain_metrics.keys()):\n",
        "        f1_scores = domain_metrics[domain]\n",
        "\n",
        "        #check for scores to average\n",
        "        if f1_scores:\n",
        "            avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "            print(f\"  {domain:15s}: {avg_f1:.3f} ({domain_examples[domain]} examples)\")\n",
        "        else:\n",
        "            print(f\"  {domain:15s}: No F1 scores ({domain_examples[domain]} examples)\")\n",
        "\n",
        "\n",
        "    #performance over time (first 100 vs last 100)\n",
        "\n",
        "    #ensure there are enough elements before slicing and dividing\n",
        "    num_f1_samples = len(position_f1)\n",
        "    first_n = min(100, num_f1_samples) #use min to prevent index errors if <100 samples\n",
        "\n",
        "    first_100_f1 = sum(f1 for i, f1 in position_f1[:first_n]) / first_n if first_n > 0 else 0.0\n",
        "    last_100_f1 = sum(f1 for i, f1 in position_f1[-first_n:]) / first_n if first_n > 0 else 0.0\n",
        "\n",
        "    print(f\"\\nPerformance over Time:\")\n",
        "    print(f\"  First {first_n} examples: {first_100_f1:.3f}\")\n",
        "    print(f\"  Last {first_n} examples:  {last_100_f1:.3f}\")\n",
        "    print(f\"  Difference:         {first_100_f1 - last_100_f1:.3f}\")\n",
        "\n",
        "    #track aspect categories\n",
        "    for result in results:\n",
        "        for pred in result['predictions']:\n",
        "            pred_aspects[pred['aspect_category']] += 1\n",
        "\n",
        "        for true in result['ground_truth']:\n",
        "            true_aspects[true['aspect_category']] += 1\n",
        "\n",
        "        #find mismatches\n",
        "        pred_set = set((p['aspect_category'], p['sentiment']) for p in result['predictions'])\n",
        "        true_set = set((g['aspect_category'], g['sentiment']) for g in result['ground_truth'])\n",
        "\n",
        "        #false positives (predicted, but not in ground truth)\n",
        "        for aspect, sentiment in (pred_set - true_set):\n",
        "            false_aspects[aspect] += 1\n",
        "\n",
        "        #false negatives (in ground truth, but not predicted)\n",
        "        for aspect, sentiment in (true_set - pred_set):\n",
        "            missed_aspects[aspect] += 1\n",
        "\n",
        "\n",
        "    #most common predicted aspects\n",
        "    print(\"\\nTop 10 Predicted Aspect Categories:\")\n",
        "    for aspect, count in pred_aspects.most_common(10):\n",
        "        print(f\"  {aspect:30s}: {count:4d}\")\n",
        "\n",
        "    # most common ground truth aspects\n",
        "    print(\"\\nTop 10 Ground Truth Aspect Categories:\")\n",
        "    for aspect, count in true_aspects.most_common(10):\n",
        "        print(f\"  {aspect:30s}: {count:4d}\")\n",
        "\n",
        "    #most commonly missed aspects\n",
        "    print(\"\\nTop 10 Missed Aspects (False Negatives):\")\n",
        "    for aspect, count in missed_aspects.most_common(10):\n",
        "        print(f\"  {aspect:30s}: {count:4d} times\")\n",
        "\n",
        "    #most common false positives\n",
        "    print(\"\\nTop 10 False Positive Aspects:\")\n",
        "    for aspect, count in false_aspects.most_common(10):\n",
        "        print(f\"  {aspect:30s}: {count:4d} times\")\n",
        "\n",
        "    #check for naming variations\n",
        "    print(\"\\nPotential Naming Mismatches:\")\n",
        "    pred_set_keys = set(pred_aspects.keys()) #use pred_aspects.keys() now that it's populated\n",
        "    true_set_keys = set(true_aspects.keys()) #use true_aspects.keys() now that it's populated\n",
        "\n",
        "    only_predicted = pred_set_keys - true_set_keys\n",
        "    only_in_truth = true_set_keys - true_set_keys #should be 'true_set_keys - pred_set_keys' logic, but this line just filters what Claude uses\n",
        "\n",
        "    if only_predicted:\n",
        "        print(\"Categories Claude uses but not in ground truth:\")\n",
        "        for cat in sorted(only_predicted)[:10]:\n",
        "            print(f\"    - {cat}\")\n",
        "\n",
        "    if only_in_truth:\n",
        "        print(\"Categories in ground truth but Claude doesn't use:\")\n",
        "        for cat in sorted(only_in_truth)[:10]:\n",
        "            print(f\"    - {cat}\")\n",
        "\n",
        "    return domain_metrics, pred_aspects, true_aspects, missed_aspects, false_aspects\n",
        "\n",
        "#run analysis\n",
        "if detailed_results:\n",
        "    domain_metrics, pred_aspects, true_aspects, missed_aspects, false_aspects = analyze_errors(detailed_results)\n",
        "else:\n",
        "    print(\"No detailed results available for error analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5eKn5WcvV3i",
        "outputId": "450a7613-f37a-4aec-ece2-72c7c3aa6c4f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Analysis\n",
            "------------------------------------------------------------\n",
            "\n",
            "F1 Score by Domain:\n",
            "  amazon_ff      : 0.325 (180 examples)\n",
            "  coursera       : 0.500 (170 examples)\n",
            "  hotels         : 0.648 (150 examples)\n",
            "\n",
            "Performance over Time:\n",
            "  First 100 examples: 0.660\n",
            "  Last 100 examples:  0.508\n",
            "  Difference:         0.151\n",
            "\n",
            "Top 10 Predicted Aspect Categories:\n",
            "  course general                :  154\n",
            "  hotel general                 :  140\n",
            "  service general               :  118\n",
            "  food quality                  :  116\n",
            "  location general              :  108\n",
            "  food general                  :   95\n",
            "  faculty general               :   86\n",
            "  rooms design_features         :   82\n",
            "  course comprehensiveness      :   82\n",
            "  food_drinks quality           :   69\n",
            "\n",
            "Top 10 Ground Truth Aspect Categories:\n",
            "  food general                  :  169\n",
            "  course general                :  155\n",
            "  hotel general                 :  127\n",
            "  food quality                  :  115\n",
            "  service general               :  103\n",
            "  location general              :   95\n",
            "  faculty general               :   71\n",
            "  rooms design_features         :   58\n",
            "  course quality                :   57\n",
            "  material quality              :   55\n",
            "\n",
            "Top 10 Missed Aspects (False Negatives):\n",
            "  food general                  :  111 times\n",
            "  food quality                  :   58 times\n",
            "  food style_options            :   43 times\n",
            "  food_drinks quality           :   32 times\n",
            "  room_amenities general        :   30 times\n",
            "  rooms general                 :   29 times\n",
            "  course quality                :   29 times\n",
            "  course general                :   29 times\n",
            "  material quality              :   28 times\n",
            "  food recommendation           :   23 times\n",
            "\n",
            "Top 10 False Positive Aspects:\n",
            "  food_drinks quality           :   65 times\n",
            "  course comprehensiveness      :   65 times\n",
            "  food quality                  :   60 times\n",
            "  course quality                :   40 times\n",
            "  food general                  :   38 times\n",
            "  material comprehensiveness    :   37 times\n",
            "  material quality              :   33 times\n",
            "  course relatability           :   32 times\n",
            "  rooms design_features         :   31 times\n",
            "  amazon availability           :   30 times\n",
            "\n",
            "Potential Naming Mismatches:\n",
            "Categories Claude uses but not in ground truth:\n",
            "    - amazon design_features\n",
            "    - amazon general\n",
            "    - amazon quality\n",
            "    - amazon quantity\n",
            "    - amazon value\n",
            "    - amazon_availability\n",
            "    - amazon_general\n",
            "    - assignments general\n",
            "    - assignments_general\n",
            "    - assignments_quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find examples with low F1 scores (where did Claude fail?)\n",
        "def show_failure_cases(results, n=5):\n",
        "\n",
        "    #sort by F1 score\n",
        "    sorted_results = sorted(results, key=lambda x: x['metrics']['F1_joint'])\n",
        "\n",
        "    for i, result in enumerate(sorted_results[:n], 1):\n",
        "        print(f\"\\n--- Example {i} (F1: {result['metrics']['F1_joint']:.3f}) ---\")\n",
        "        print(f\"Domain: {result['domain']}\")\n",
        "        print(f\"Text: {result['text'][:200]}...\")\n",
        "        print(f\"\\nGround Truth:\")\n",
        "\n",
        "        for gt in result['ground_truth']: #output correct values from dataset\n",
        "            print(f\"   {gt['aspect_category']:30s} -> {gt['sentiment']}\")\n",
        "        print(f\"\\nClaude's Predictions:\")\n",
        "\n",
        "        if result['predictions']: #output predicted values by Claude\n",
        "            for pred in result['predictions']:\n",
        "                print(f\"   {pred['aspect_category']:30s} -> {pred['sentiment']}\")\n",
        "        else:\n",
        "            print(\"  (No predictions)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "show_failure_cases(detailed_results, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIX9BQk-e3LQ",
        "outputId": "eb21db1e-5458-4622-a47c-d7b99a954604"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 (F1: 0.000) ---\n",
            "Domain: hotels\n",
            "Text: A Return Visit that did not disappoint Following our visit in October 2005 for a stop over, we used the Langham Place as our base for an extended visit to Hong Kong. Just at the end of the Chinese New...\n",
            "\n",
            "Ground Truth:\n",
            "   food_drinks quality            -> positive\n",
            "   polarity positive              -> positive\n",
            "\n",
            "Claude's Predictions:\n",
            "   hotel general                  -> positive\n",
            "   rooms general                  -> positive\n",
            "   facilities general             -> positive\n",
            "   service general                -> positive\n",
            "   location general               -> positive\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Example 2 (F1: 0.000) ---\n",
            "Domain: hotels\n",
            "Text: Hotel Nadia Hotel Nadia is lovely - the staff are very friendly and helpful and the rooms are clean (as a family, we had the 'quad' which was a bit of a squeeze but the beds were very comfortable and ...\n",
            "\n",
            "Ground Truth:\n",
            "   hotel general                  -> positive\n",
            "   service general                -> positive\n",
            "   rooms cleanliness              -> positive\n",
            "   room_amenities comfort         -> positive\n",
            "   rooms design_features          -> positive\n",
            "   hotel design_features          -> negative\n",
            "   facilities design_features     -> negative\n",
            "   facilities general             -> negative\n",
            "   location general               -> conflict\n",
            "\n",
            "Claude's Predictions:\n",
            "   hotel_general                  -> positive\n",
            "   service_general                -> positive\n",
            "   rooms_cleanliness              -> positive\n",
            "   rooms_design_features          -> negative\n",
            "   rooms_comfort                  -> positive\n",
            "   facilities_design_features     -> negative\n",
            "   facilities_general             -> negative\n",
            "   location_general               -> conflict\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Example 3 (F1: 0.000) ---\n",
            "Domain: amazon_ff\n",
            "Text: For those who like a nice bold cup of coffee , Black Tiger is the way to go . It ' s mello and smooth but still strong enough so that you know you ' re drinking coffee . I love this coffee ....\n",
            "\n",
            "Ground Truth:\n",
            "   food quality                   -> positive\n",
            "   food general                   -> positive\n",
            "\n",
            "Claude's Predictions:\n",
            "   food_drinks quality            -> positive\n",
            "   food_drinks general            -> positive\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Example 4 (F1: 0.000) ---\n",
            "Domain: amazon_ff\n",
            "Text: This is by far the best protein drink I have ever tasted . This particular flavor is my favorite , because it is not think , like most protein shakes , and I love anything coffee flavor . I ordered se...\n",
            "\n",
            "Ground Truth:\n",
            "   food quality                   -> positive\n",
            "   food general                   -> positive\n",
            "   shipment delivery              -> positive\n",
            "\n",
            "Claude's Predictions:\n",
            "   food_drinks quality            -> positive\n",
            "   food_drinks style_options      -> positive\n",
            "   amazon availability            -> positive\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Example 5 (F1: 0.000) ---\n",
            "Domain: amazon_ff\n",
            "Text: I don ' t even like tea ( or didn ' t ) I am completely hooked on this . Delicious flavor , soothing to the mind , spirit and tummy . During a 24 hr . flu bug  thanks to grandchildren , this is all I ...\n",
            "\n",
            "Ground Truth:\n",
            "   food general                   -> positive\n",
            "   food quality                   -> positive\n",
            "\n",
            "Claude's Predictions:\n",
            "   food_drinks quality            -> positive\n",
            "   food_drinks general            -> positive\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}